import os
import re
from html.parser import HTMLParser
from difflib import get_close_matches

class HTMLTextExtractor(HTMLParser):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML —Ñ–∞–π–ª–æ–≤"""
    def __init__(self):
        super().__init__()
        self.text = []
    
    def handle_data(self, data):
        self.text.append(data)
    
    def get_text(self):
        return ' '.join(self.text)

class TeacherBot:
    """
    TeacherBot - –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–∞—Ö
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç TXT –∏ HTML —Ñ–∞–π–ª—ã, –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    """
    
    def __init__(self, knowledge_base_path='knowledge_base'):
        self.knowledge_base_path = knowledge_base_path
        self.documents = []
        self.terms_index = {}
        self.load_knowledge_base()
    
    def load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ knowledge_base"""
        if not os.path.exists(self.knowledge_base_path):
            print(f"Warning: Knowledge base folder '{self.knowledge_base_path}' not found")
            return
        
        self.documents = []
        self.terms_index = {}
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        for root, dirs, files in os.walk(self.knowledge_base_path):
            for filename in files:
                if filename.endswith(('.txt', '.html', '.htm')):
                    filepath = os.path.join(root, filename)
                    try:
                        content = self._read_file(filepath)
                        if content:
                            doc = {
                                'filename': filename,
                                'filepath': filepath,
                                'content': content
                            }
                            self.documents.append(doc)
                            self._index_document(doc)
                    except Exception as e:
                        print(f"Error reading {filepath}: {e}")
        
        print(f"Loaded {len(self.documents)} documents from knowledge base")
    
    def _read_file(self, filepath):
        """–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ (TXT –∏–ª–∏ HTML)"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ï—Å–ª–∏ HTML, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            if filepath.endswith(('.html', '.htm')):
                parser = HTMLTextExtractor()
                parser.feed(content)
                content = parser.get_text()
            
            return content
        except UnicodeDecodeError:
            # –ü–æ–ø—ã—Ç–∫–∞ —Å –¥—Ä—É–≥–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
            try:
                with open(filepath, 'r', encoding='cp1251') as f:
                    content = f.read()
                if filepath.endswith(('.html', '.htm')):
                    parser = HTMLTextExtractor()
                    parser.feed(content)
                    content = parser.get_text()
                return content
            except:
                return None
    
    def _index_document(self, doc):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        words = re.findall(r'\w+', doc['content'].lower())
        for word in set(words):
            if word not in self.terms_index:
                self.terms_index[word] = []
            self.terms_index[word].append(doc)
    
    def search(self, query):
        """
        –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        if not self.documents:
            return "üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –î–æ–±–∞–≤—å—Ç–µ TXT –∏–ª–∏ HTML —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É 'knowledge_base'."
        
        query_lower = query.lower().strip()
        
        # –£–±–∏—Ä–∞–µ–º –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        query_clean = re.sub(r'^(—á—Ç–æ —Ç–∞–∫–æ–µ|—á—Ç–æ —ç—Ç–æ|–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|define|explain|—á—Ç–æ)\s+', '', query_lower)
        query_clean = query_clean.replace('?', '').strip()
        
        # –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        results = self._search_exact(query_clean)
        
        if results:
            return self._format_results(results, query_clean)
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        similar = self._find_similar_terms(query_clean)
        if similar:
            suggestions = ', '.join(similar[:5])
            return f"üîç –¢–µ—Ä–º–∏–Ω '{query_clean}' –Ω–µ –Ω–∞–π–¥–µ–Ω.\n\n–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏—Å–∫–∞–ª–∏: {suggestions}?"
        
        return f"‚ùå –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ '{query_clean}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏–µ."
    
    def _search_exact(self, term):
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–∞"""
        results = []
        term_words = term.split()
        
        for doc in self.documents:
            content_lower = doc['content'].lower()
            
            # –ü–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ
            if term in content_lower:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                context = self._extract_context(doc['content'], term)
                results.append({
                    'filename': doc['filename'],
                    'context': context
                })
        
        return results
    
    def _extract_context(self, content, term, context_size=300):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞"""
        content_lower = content.lower()
        term_lower = term.lower()
        
        pos = content_lower.find(term_lower)
        if pos == -1:
            return None
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        start = max(0, pos - context_size)
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_start = content.rfind('.', start, pos)
        if sentence_start != -1 and sentence_start > start:
            start = sentence_start + 1
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        end = min(len(content), pos + len(term) + context_size)
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_end = content.find('.', pos + len(term), end)
        if sentence_end != -1:
            end = sentence_end + 1
        
        context = content[start:end].strip()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω
        if start > 0:
            context = '...' + context
        if end < len(content):
            context = context + '...'
        
        return context
    
    def _find_similar_terms(self, term):
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è)"""
        all_terms = list(self.terms_index.keys())
        similar = get_close_matches(term, all_terms, n=5, cutoff=0.6)
        return similar
    
    def _format_results(self, results, query):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        if not results:
            return f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        
        response = f"üìñ **{query.upper()}**\n\n"
        
        for i, result in enumerate(results[:3], 1):  # –ú–∞–∫—Å–∏–º—É–º 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            response += f"üìÑ –ò–∑ —Ñ–∞–π–ª–∞: {result['filename']}\n"
            response += f"{result['context']}\n\n"
            if i < len(results):
                response += "---\n\n"
        
        if len(results) > 3:
            response += f"\n... –∏ –µ—â—ë {len(results) - 3} —Ä–µ–∑—É–ª—å—Ç–∞—Ç(–æ–≤)"
        
        return response.strip()
    
    def get_all_terms(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        return sorted(list(self.terms_index.keys()))
    
    def reload(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        self.load_knowledge_base()
        return f"–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == '__main__':
    bot = TeacherBot()
    
    # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
    test_queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ IP –∞–¥—Ä–µ—Å?",
        "–∏–Ω—Ç–µ–≥—Ä–∞–ª",
        "—Ñ–æ—Ä–º—É–ª–∞",
    ]
    
    print("TeacherBot Test")
    print("=" * 50)
    for query in test_queries:
        print(f"\n–ó–∞–ø—Ä–æ—Å: {query}")
        print(bot.search(query))
        print("-" * 50)
